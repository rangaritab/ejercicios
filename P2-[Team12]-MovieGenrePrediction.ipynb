{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "P2-[Team12]-MovieGenrePrediction.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "colab_type": "text",
        "id": "gl0eYRqbPcqN"
      },
      "cell_type": "markdown",
      "source": [
        "# Project 2\n",
        "\n",
        "\n",
        "# Movie Genre Classification\n",
        "\n",
        "## API: http://18.216.214.146:8888/\n",
        "\n",
        "## Team12 (Grupo en Kaggle)\n",
        "\n",
        "#### Angélica Viviana Parrado Cubillos\n",
        "#### Robert Angarita Bermúdez"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "RA0F0IFvPcqQ"
      },
      "cell_type": "markdown",
      "source": [
        "## Clasificación de texto"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "moFKOQcXPcqU"
      },
      "cell_type": "markdown",
      "source": [
        "## Introducción\n",
        "\n",
        "La clasificación de texto es una de las tareas más importantes en el procesamiento del lenguaje natural. Es el proceso de clasificación de cadenas de texto o documentos en diferentes categorías, dependiendo del contenido de las cadenas. La clasificación de texto tiene una variedad de aplicaciones, como detectar el sentimiento del usuario a partir de un tweet, clasificar un correo electrónico como correo no deseado o spam, clasificar publicaciones de blogs en diferentes categorías, etiquetar automáticamente las consultas de los clientes, etc.\n",
        "\n",
        "El presente proyecto tiene como objetivo la clasificación de texto en el mundo real. Entrenaremos un modelo de aprendizaje automático capaz de predecir a qué género pertenece la película dado la synapsis. Este es un proyecto de análisis para clasificar un género en particular."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "I6HjBeBSPcqW"
      },
      "cell_type": "markdown",
      "source": [
        "### Conjunto de datos\n",
        "\n",
        "El conjunto de datos de entrenamiento que vamos a utilizar para este proyecto corresponde a los títulos, reseñas y genero. El conjunto de datos consta de un total de 7895 registros."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "knLZu0FlPcqY"
      },
      "cell_type": "markdown",
      "source": [
        "### Clasificación del genero para películas\n",
        "\n",
        "En este proyecto, realizaremos una serie de pasos necesarios para predecir los géneros de las reseñas de diferentes películas. Estos pasos se pueden utilizar para cualquier tarea de clasificación de texto. Utilizaremos la biblioteca Scikit-Learn de Python para el aprendizaje automático para entrenar un modelo de clasificación de texto.\n",
        "\n",
        "Los siguientes son los pasos necesarios para crear un modelo de clasificación de texto en Python:\n",
        "\n",
        "1. Importando Bibliotecas\n",
        "2. Importando el conjunto de datos\n",
        "3. Preprocesamiento de texto\n",
        "4. CountVectorizer\n",
        "5. Conjuntos de entrenamiento y prueba\n",
        "6. Modelo de clasificación de textos de entrenamiento y predicción de sentimientos"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "xO7JQstBPcqb"
      },
      "cell_type": "markdown",
      "source": [
        "### 1. Importando Bibliotecas\n",
        "Ejecutamos el siguiente script para importar las bibliotecas requeridas:"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "okjBMyZkPcqd",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.metrics import r2_score, roc_auc_score\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "5twiyGpdPcrD"
      },
      "cell_type": "markdown",
      "source": [
        "### 2. Importando el conjunto de datos\n",
        "\n",
        "Utilizaremos la función *PANDAS* de la biblioteca sklearn_datasets para importar el conjunto de datos a nuestra aplicación. La función toma el nombre asignado a cada conjunto de datos en conjuntos de datos y objetivos. Por ejemplo, en nuestro caso,  la ruta del directorio \"dataTraining.csv y dataTesting.csv\" debe ser la misma donde se está ejecutando el *NOTEBOOK JUPYTER*. Los pandas tratarán cada carpeta dentro de la misma carpeta como una categoría y a todos los documentos dentro de esa carpeta se les asignará su categoría correspondiente, de entrenamiento o testeo.\n",
        "\n",
        "Ejecutamos el siguiente script para ver la función pandas en acción:"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "yjPtkf-BPcrH",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dataTraining = pd.read_csv('https://github.com/albahnsen/PracticalMachineLearningClass/raw/master/datasets/dataTraining.zip', encoding='UTF-8', index_col=0)\n",
        "dataTesting = pd.read_csv('https://github.com/albahnsen/PracticalMachineLearningClass/raw/master/datasets/dataTesting.zip', encoding='UTF-8', index_col=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "EbRnwF7APcrP"
      },
      "cell_type": "markdown",
      "source": [
        "En el script anterior se cargan los dos dataframe de entrenamiento y test."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "vIsTwS_RPcrS"
      },
      "cell_type": "markdown",
      "source": [
        "### 3. Preprocesamiento de texto\n",
        "\n",
        "Una vez que se haya importado el conjunto de datos, el siguiente paso es preprocesar el texto. Ejecute el siguiente script para preprocesar los datos:"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ibs2VqBdPcrV",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from nltk.stem.snowball import SnowballStemmer\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "def stemming(sentence):\n",
        "    stemSentence = \"\"\n",
        "    for word in sentence.split():\n",
        "        stem = stemmer.stem(word)\n",
        "        stemSentence += stem\n",
        "        stemSentence += \" \"\n",
        "    stemSentence = stemSentence.strip()\n",
        "    return stemSentence\n",
        "  \n",
        "dataTraining['plot'] = dataTraining['plot'].apply(stemming)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "84yuDG48Pcrc"
      },
      "cell_type": "markdown",
      "source": [
        "Steamming y lemmatization son los métodos básicos de procesamiento de texto. El objetivo de la derivación de éstas, es reducir las formas de inflexión y, a veces las formas relacionadas de una palabra a una base común.\n",
        "Utilizamos el metodo Steamming con Snowball que es un lenguaje de procesamiento de cadenas pequeñas diseñado para crear algoritmos de derivación para su uso en la recuperación de información."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "tvvC-2QyPcrq"
      },
      "cell_type": "markdown",
      "source": [
        "### 4. Creamos Tfidfvectorizer"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "q1bilImCPcrs",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import re, string\n",
        "re_tok = re.compile(f'([{string.punctuation}])')\n",
        "def tokenize(s): return re_tok.sub(r' \\1 ', s).split()\n",
        "\n",
        "vect = TfidfVectorizer(input=\"content\", \n",
        "                encoding=\"utf-8\", \n",
        "                decode_error=\"strict\", \n",
        "                strip_accents=\"unicode\", \n",
        "                lowercase=True, \n",
        "                preprocessor=None, \n",
        "                tokenizer=tokenize, \n",
        "                analyzer=\"word\", \n",
        "                stop_words=None, \n",
        "                token_pattern=\"(?u)\\b\\w\\w+\\b\", \n",
        "                ngram_range=(1, 2), \n",
        "                max_df=0.9, \n",
        "                min_df=3, \n",
        "                max_features=None, \n",
        "                vocabulary=None, \n",
        "                binary=False, \n",
        "                norm=\"l2\", \n",
        "                use_idf=1, \n",
        "                smooth_idf=1, \n",
        "                sublinear_tf=1)\n",
        "\n",
        "X_dtm = vect.fit_transform(dataTraining['plot'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "cCjWHkgbPcr0"
      },
      "cell_type": "markdown",
      "source": [
        "El script anterior utiliza la clase TfidfVectorizer de la biblioteca sklearn.feature_extraction.text. Hay algunos parámetros importantes que deben pasarse al constructor de la clase. Todas las palabras únicas en todos los documentos se convierten en características. Todos los documentos pueden contener decenas de miles de palabras únicas. Pero las palabras que tienen una frecuencia de aparición muy baja no son, en general, un buen parámetro para clasificar documentos.\n",
        "\n",
        "La función fit_transform de la clase TfidfVectorizer convierte los documentos de texto en las características numéricas correspondientes."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "5vYas1O9Pcr6"
      },
      "cell_type": "markdown",
      "source": [
        "#### Creamos la variable Y"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "jsRP-4hyPcr-",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dataTraining['genres'] = dataTraining['genres'].map(lambda x: eval(x))\n",
        "le = MultiLabelBinarizer()\n",
        "y_genres = le.fit_transform(dataTraining['genres'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "RwX_zuUjPcsL",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train_genres, y_test_genres = train_test_split(X_dtm, y_genres,test_size=0.2,random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Sg58EkTZPcsJ"
      },
      "cell_type": "markdown",
      "source": [
        "### 5. Conjuntos de entrenamiento y pruebas\n",
        "\n",
        "Al igual que cualquier otro problema de aprendizaje automático supervisado, debemos dividir nuestros datos en conjuntos de capacitación y pruebas. Para hacerlo, usaremos la utilidad train_test_split de la biblioteca sklearn.model_selection . Ejecutamos el siguiente script:"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "xVqv6UpNPcsm"
      },
      "cell_type": "markdown",
      "source": [
        "El script anterior divide los datos en 20% de conjunto de prueba y 80% de conjunto de entrenamiento."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "ZZP2zFHaPcsq"
      },
      "cell_type": "markdown",
      "source": [
        "### 6. Modelo de clasificación de textos de entrenamiento y predicción de género\n",
        "\n",
        "Hemos dividido nuestros datos en conjunto de entrenamiento y pruebas. Ahora es el momento de ver la predicción. Usaremos el algoritmo de regresión logística para entrenar nuestro modelo.\n",
        "\n",
        "Para este notebook del proyecto *Movie Genre Classification* con la parametrización específica.\n",
        "\n",
        "Los clasificadores son:\n",
        "\n",
        "    1) Regresión Logística\n",
        "\n",
        "El método de fit de esta clase se utiliza para entrenar el algoritmo. Necesitamos pasar los datos de entrenamiento y los conjuntos de objetivos de entrenamiento a este método. Echa un vistazo a la siguiente secuencia de comandos:"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "C22AbgsbPcst",
        "outputId": "a7f08f09-8c87-4889-ed7d-65c4c2f2ad4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "clf = OneVsRestClassifier(LogisticRegression(penalty=\"l2\", \n",
        "                                             dual=False, \n",
        "                                             tol=0.0001, \n",
        "                                             C=1.0, \n",
        "                                             fit_intercept=True, \n",
        "                                             intercept_scaling=1, \n",
        "                                             class_weight=\"balanced\", \n",
        "                                             random_state=None, \n",
        "                                             solver=\"liblinear\", \n",
        "                                             max_iter=100, \n",
        "                                             multi_class=\"auto\", \n",
        "                                             verbose=0, \n",
        "                                             warm_start=False, \n",
        "                                             n_jobs=None))\n",
        "\n",
        "clf.fit(X_train, y_train_genres)    "
      ],
      "execution_count": 274,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 4.79 s, sys: 2.98 ms, total: 4.79 s\n",
            "Wall time: 4.8 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "6L5v_BeMPcs3"
      },
      "cell_type": "markdown",
      "source": [
        "Finalmente, para predecir los géneros de los conjunto de prueba, podemos usar el método de predict de la clase *Regresión Logistica (LogisticRegression)* como se muestra a continuación:"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "-Hq4oWKtPcs6",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y_pred_genres = clf.predict_proba(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Wev_lNWEPctc"
      },
      "cell_type": "markdown",
      "source": [
        "Se ha entrenado con éxito el modelo de clasificación LOGÍSTICA de texto y ha realizado todas predicciones. A continuación es el momento de ver el rendimiento del modelo que acabamos de desarrollar."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "cLige-ODPctf",
        "outputId": "36c7820f-aad4-4618-cf4f-4bdd14ec050d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "print(\"AUC Score = \", roc_auc_score(y_test_genres, y_pred_genres, average='macro')) "
      ],
      "execution_count": 276,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "AUC Score =  0.897171369225946\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "d9L0O5q2Pctq"
      },
      "cell_type": "markdown",
      "source": [
        "Predecimos *the testing datasets*"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "q7T-Fvx9Pcts",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_test_dtm = vect.transform(dataTesting['plot'])\n",
        "\n",
        "cols = ['p_Action', 'p_Adventure', 'p_Animation', 'p_Biography', 'p_Comedy', 'p_Crime', 'p_Documentary', 'p_Drama', 'p_Family',\n",
        "        'p_Fantasy', 'p_Film-Noir', 'p_History', 'p_Horror', 'p_Music', 'p_Musical', 'p_Mystery', 'p_News', 'p_Romance',\n",
        "        'p_Sci-Fi', 'p_Short', 'p_Sport', 'p_Thriller', 'p_War', 'p_Western']\n",
        "\n",
        "y_pred_test_genres = clf.predict_proba(X_test_dtm)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "mISv4MKOPct0",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "res = pd.DataFrame(y_pred_test_genres, index=dataTesting.index, columns=cols)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "5LN0ztdQPcuJ",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
