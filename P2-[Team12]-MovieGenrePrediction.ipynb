{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copia de P2-[Team12]-MovieGenrePrediction-Copy1.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "gl0eYRqbPcqN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Project 2\n",
        "\n",
        "\n",
        "# Movie Genre Classification\n",
        "\n",
        "## API: http://18.216.214.146:8888/\n",
        "\n",
        "## Team12 (Grupo en Kaggle)\n",
        "\n",
        "#### Angélica Viviana Parrado Cubillos\n",
        "#### Robert Angarita Bermúdez"
      ]
    },
    {
      "metadata": {
        "id": "RA0F0IFvPcqQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Clasificación de texto"
      ]
    },
    {
      "metadata": {
        "id": "moFKOQcXPcqU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Introducción\n",
        "\n",
        "La clasificación de texto es una de las tareas más importantes en el procesamiento del lenguaje natural. Es el proceso de clasificación de cadenas de texto o documentos en diferentes categorías, dependiendo del contenido de las cadenas. La clasificación de texto tiene una variedad de aplicaciones, como detectar el sentimiento del usuario a partir de un tweet, clasificar un correo electrónico como correo no deseado o spam, clasificar publicaciones de blogs en diferentes categorías, etiquetar automáticamente las consultas de los clientes, etc.\n",
        "\n",
        "El presente proyecto tiene como objetivo la clasificación de texto en el mundo real. Entrenaremos un modelo de aprendizaje automático capaz de predecir a qué género pertenece la película dado la synapsis. Este es un proyecto de análisis para clasificar un género en particular."
      ]
    },
    {
      "metadata": {
        "id": "I6HjBeBSPcqW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Conjunto de datos\n",
        "\n",
        "El conjunto de datos de entrenamiento que vamos a utilizar para este proyecto corresponde a los títulos, reseñas y genero. El conjunto de datos consta de un total de 7895 registros. La mitad de los documentos contienen comentarios positivos sobre una película, mientras que la mitad restante contiene comentarios negativos. Más detalles sobre el conjunto de datos se pueden encontrar en este enlace ."
      ]
    },
    {
      "metadata": {
        "id": "knLZu0FlPcqY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Clasificación del genero para películas\n",
        "\n",
        "En este proyecto, realizaremos una serie de pasos necesarios para predecir los géneros de las reseñas de diferentes películas. Estos pasos se pueden utilizar para cualquier tarea de clasificación de texto. Utilizaremos la biblioteca Scikit-Learn de Python para el aprendizaje automático para entrenar un modelo de clasificación de texto.\n",
        "\n",
        "Los siguientes son los pasos necesarios para crear un modelo de clasificación de texto en Python:\n",
        "\n",
        "1. Importando Bibliotecas\n",
        "2. Importando el conjunto de datos\n",
        "3. Preprocesamiento de texto\n",
        "4. CountVectorizer\n",
        "5. Conjuntos de entrenamiento y prueba\n",
        "6. Modelo de clasificación de textos de entrenamiento y predicción de sentimientos"
      ]
    },
    {
      "metadata": {
        "id": "xO7JQstBPcqb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 1. Importando Bibliotecas\n",
        "Ejecutamos el siguiente script para importar las bibliotecas requeridas:"
      ]
    },
    {
      "metadata": {
        "id": "okjBMyZkPcqd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, ExtraTreesClassifier\n",
        "from sklearn.metrics import r2_score, roc_auc_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "import xgboost as xgb\n",
        "import sklearn.model_selection"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5twiyGpdPcrD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2. Importando el conjunto de datos\n",
        "\n",
        "Utilizaremos la función *PANDAS* de la biblioteca sklearn_datasets para importar el conjunto de datos a nuestra aplicación. La función toma el nombre asignado a cada conjunto de datos en conjuntos de datos y objetivos. Por ejemplo, en nuestro caso,  la ruta del directorio \"dataTraining.csv y dataTesting.csv\" debe ser la misma donde se está ejecutando el *NOTEBOOK JUPYTER*. Los pandas tratarán cada carpeta dentro de la misma carpeta como una categoría y a todos los documentos dentro de esa carpeta se les asignará su categoría correspondiente, de entrenamiento o testeo.\n",
        "\n",
        "Ejecutamos el siguiente script para ver la función pandas en acción:"
      ]
    },
    {
      "metadata": {
        "id": "yjPtkf-BPcrH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dataTraining = pd.read_csv('https://github.com/albahnsen/PracticalMachineLearningClass/raw/master/datasets/dataTraining.zip', encoding='UTF-8', index_col=0)\n",
        "dataTesting = pd.read_csv('https://github.com/albahnsen/PracticalMachineLearningClass/raw/master/datasets/dataTesting.zip', encoding='UTF-8', index_col=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EbRnwF7APcrP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "En el script anterior se cargan los dos dataframe de entrenamiento y test."
      ]
    },
    {
      "metadata": {
        "id": "vIsTwS_RPcrS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 3. Preprocesamiento de texto\n",
        "\n",
        "Una vez que se haya importado el conjunto de datos, el siguiente paso es preprocesar el texto. Ejecute el siguiente script para preprocesar los datos:"
      ]
    },
    {
      "metadata": {
        "id": "ibs2VqBdPcrV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from nltk.stem.snowball import SnowballStemmer\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "def stemming(sentence):\n",
        "    stemSentence = \"\"\n",
        "    for word in sentence.split():\n",
        "        stem = stemmer.stem(word)\n",
        "        stemSentence += stem\n",
        "        stemSentence += \" \"\n",
        "    stemSentence = stemSentence.strip()\n",
        "    return stemSentence\n",
        "dataTraining['plot'] = dataTraining['plot'].apply(stemming)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "84yuDG48Pcrc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Steamming y lemmatization son los métodos básicos de procesamiento de texto. El objetivo de la derivación de éstas, es reducir las formas de inflexión y, a veces las formas relacionadas de una palabra a una base común.\n",
        "Utilizamos el metodo Steamming con Snowball que es un lenguaje de procesamiento de cadenas pequeñas diseñado para crear algoritmos de derivación para su uso en la recuperación de información."
      ]
    },
    {
      "metadata": {
        "id": "tvvC-2QyPcrq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 4. Creamos count vectorizer"
      ]
    },
    {
      "metadata": {
        "id": "q1bilImCPcrs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import re, string\n",
        "re_tok = re.compile(f'([{string.punctuation}“”¨«»®´·º½¾¿¡§£₤‘’])')\n",
        "def tokenize(s): return re_tok.sub(r' \\1 ', s).split()\n",
        "vect = TfidfVectorizer(ngram_range=(1,2), tokenizer=tokenize,\n",
        "               min_df=1, max_df=0.4, strip_accents='unicode', use_idf=1,\n",
        "               smooth_idf=1, sublinear_tf=1 )\n",
        "X_dtm = vect.fit_transform(dataTraining['plot'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cCjWHkgbPcr0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "El script anterior utiliza la clase CountVectorizer de la biblioteca sklearn.feature_extraction.text . Hay algunos parámetros importantes que deben pasarse al constructor de la clase. El primer parámetro es el parámetro max_features , que se establece en 11000. Esto se debe a que cuando convierte palabras en números utilizando la bolsa de palabras, todas las palabras únicas en todos los documentos se convierten en características. Todos los documentos pueden contener decenas de miles de palabras únicas. Pero las palabras que tienen una frecuencia de aparición muy baja no son, en general, un buen parámetro para clasificar documentos. Por lo tanto, establecemos el parámetro max_features en 11000, lo que significa que queremos usar las 11000 palabras más comunes como características para entrenar a nuestro clasificador.\n",
        "\n",
        "La función fit_transform de la clase CountVectorizer convierte los documentos de texto en las características numéricas correspondientes."
      ]
    },
    {
      "metadata": {
        "id": "5vYas1O9Pcr6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Creamos la variable Y"
      ]
    },
    {
      "metadata": {
        "id": "jsRP-4hyPcr-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dataTraining['genres'] = dataTraining['genres'].map(lambda x: eval(x))\n",
        "le = MultiLabelBinarizer()\n",
        "y_genres = le.fit_transform(dataTraining['genres'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RwX_zuUjPcsL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train_genres, y_test_genres = train_test_split(X_dtm, y_genres,test_size=0.2,random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Sg58EkTZPcsJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 5. Conjuntos de entrenamiento y pruebas\n",
        "\n",
        "Al igual que cualquier otro problema de aprendizaje automático supervisado, debemos dividir nuestros datos en conjuntos de capacitación y pruebas. Para hacerlo, usaremos la utilidad train_test_split de la biblioteca sklearn.model_selection . Ejecutamos el siguiente script:"
      ]
    },
    {
      "metadata": {
        "id": "xVqv6UpNPcsm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "El script anterior divide los datos en 20% de conjunto de prueba y 80% de conjunto de entrenamiento."
      ]
    },
    {
      "metadata": {
        "id": "ZZP2zFHaPcsq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 6. Modelo de clasificación de textos de entrenamiento y predicción de género\n",
        "\n",
        "Hemos dividido nuestros datos en conjunto de entrenamiento y pruebas. Ahora es el momento de ver la acción real. Usaremos el algoritmo de bosque aleatorio para entrenar nuestro modelo. Puedes usar cualquier otro modelo de tu elección.\n",
        "\n",
        "Para este notebook del proyecto *Movie Genre Classification* con la parametrización específica.\n",
        "\n",
        "Los clasificadores son:\n",
        "\n",
        "    1) Regresión Logística\n",
        "\n",
        "El método de fit de esta clase se utiliza para entrenar el algoritmo. Necesitamos pasar los datos de entrenamiento y los conjuntos de objetivos de entrenamiento a este método. Echa un vistazo a la siguiente secuencia de comandos:"
      ]
    },
    {
      "metadata": {
        "id": "C22AbgsbPcst",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "b2350af5-d2e1-4182-cb5d-ab0cacbb2075"
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "clf = OneVsRestClassifier(LogisticRegression(C=1, dual=True,class_weight='balanced',multi_class='auto',solver='liblinear'))\n",
        "clf.fit(X_train, y_train_genres)    "
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 3.5 s, sys: 2.94 ms, total: 3.51 s\n",
            "Wall time: 3.51 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6L5v_BeMPcs3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Finalmente, para predecir los géneros de los conjunto de prueba, podemos usar el método de predict de la clase *Weighted Average Probabilities (Soft Voting)* como se muestra a continuación:"
      ]
    },
    {
      "metadata": {
        "id": "-Hq4oWKtPcs6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y_pred_genres = clf.predict_proba(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Wev_lNWEPctc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Se ha entrenado con éxito el modelo de clasificación ENSEMBED de texto y ha realizado todas predicciones. A continuación es el momento de ver el rendimiento del modelo que acabamos de desarrollar."
      ]
    },
    {
      "metadata": {
        "id": "cLige-ODPctf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c6744c0f-1bd1-4f3f-c4a2-60864ecb771b"
      },
      "cell_type": "code",
      "source": [
        "print(\"AUC Score = \", roc_auc_score(y_test_genres, y_pred_genres, average='macro')) "
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "AUC Score =  0.8964746823713585\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "d9L0O5q2Pctq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Predecimos *the testing datasets*"
      ]
    },
    {
      "metadata": {
        "id": "q7T-Fvx9Pcts",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_test_dtm = vect.transform(dataTesting['plot'])\n",
        "\n",
        "cols = ['p_Action', 'p_Adventure', 'p_Animation', 'p_Biography', 'p_Comedy', 'p_Crime', 'p_Documentary', 'p_Drama', 'p_Family',\n",
        "        'p_Fantasy', 'p_Film-Noir', 'p_History', 'p_Horror', 'p_Music', 'p_Musical', 'p_Mystery', 'p_News', 'p_Romance',\n",
        "        'p_Sci-Fi', 'p_Short', 'p_Sport', 'p_Thriller', 'p_War', 'p_Western']\n",
        "\n",
        "y_pred_test_genres = clf.predict_proba(X_test_dtm)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mISv4MKOPct0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "res = pd.DataFrame(y_pred_test_genres, index=dataTesting.index, columns=cols)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5LN0ztdQPcuJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}