# E12 - Gradient Boosting Review

Search for and comment about the main differences between the algorithms implemented in: 

(1) [ Gradient Boosting Classifier ](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html)

(2) [ XGB Classifier ](https://xgboost.readthedocs.io/en/latest/python/python_intro.html)

Gradient Boosting es otra técnica para realizar tareas de aprendizaje automático supervisadas, como la clasificación y la regresión. Las implementaciones de esta técnica pueden tener diferentes nombres, lo más común es que encuentre máquinas de Gradient Boosting (abreviadas GBM) y XGBoost. XGBoost es particularmente popular porque ha sido el algoritmo ganador en varias competiciones.
Similar a Random Forests, Gradient Boosting es un aprendiz de conjunto. Esto significa que creará un modelo final basado en una colección de modelos individuales. El poder predictivo de estos modelos individuales es débil y propenso al sobreajuste, pero la combinación de muchos de estos modelos débiles en un conjunto conducirá a un resultado general muy mejorado. En las máquinas de Gradient Boosting, el tipo más común de modelo débil utilizado son los árboles de decisión, otro paralelo a los bosques aleatorios. 
XGBoost significa Extreme Gradient Boosting; es una implementación específica del método Gradient Boosting que utiliza aproximaciones más precisas para encontrar el mejor modelo de árbol. Emplea una serie de ingeniosos trucos que lo hacen excepcionalmente exitoso, particularmente con datos estructurados. Los más importantes son:
1.) calcular gradientes de segundo orden, es decir, segundas derivadas parciales de la función de pérdida (similar al método de Newton), que proporciona más información sobre la dirección de los gradientes y cómo llegar al mínimo de nuestra función de pérdida. Mientras que el aumento de gradiente regular utiliza la función de pérdida de nuestro modelo base (por ejemplo, el árbol de decisiones) como un proxy para minimizar el error del modelo general, XGBoost usa la derivada de segundo orden como una aproximación.
2.) Y la regularización avanzada, que mejora la generalización del modelo.
XGBoost tiene ventajas adicionales: el entrenamiento es muy rápido y puede ser paralelo o distribuido.
Tanto XGBoost como Gradient Boosting siguen el principio de aumento de gradiente. Hay, sin embargo, la diferencia en los detalles de modelado. Específicamente, XGBoost utilizó una formalización de modelos más regularizada para controlar el ajuste excesivo, lo que le brinda un mejor rendimiento.
El nombre XGBoost, sin embargo, en realidad se refiere al objetivo de ingeniería para impulsar el límite de recursos de cómputo para algoritmos de árbol potenciados. Para el modelo, podría ser más adecuado llamarlo como Gradient Boosting regularizado.

