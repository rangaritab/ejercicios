# E8 - Ensemble Trees Overview

Write at least 300 words explaining why ensemble is a succesful strategy in machine learning.

Todos usamos la técnica del Árbol de Decisión a diario para planificar nuestra vida, simplemente no le damos un nombre elegante al proceso de toma de decisiones.
Las empresas utilizan estas técnicas de aprendizaje automático supervisado, como los árboles de decisión, para tomar mejores decisiones y obtener más ganancias. Los árboles de decisión han existido durante mucho tiempo y también se sabe que sufren sesgos y variaciones. Tendrá un gran sesgo con árboles simples y una gran variación con árboles complejos.
Ensemble métodos, que combinan varios árboles de decisión para producir un mejor rendimiento predictivo que utilizar un solo árbol de decisión. El principio fundamental detrás del modelo de conjunto es que un grupo de aprendices débiles se unen para formar un aprendiz fuerte.
Estos métodos se han utilizado con árboles de decisión antes, con los árboles actuando como los clasificadores débiles en el esquema de conjunto. Una de las más conocidas se propusieron técnicas de ensamble, Bagging, específicamente con árboles de decisión. Como clasificadores débiles. En el Bagging, el muestreo repetido de la correa de arranque de un conjunto de datos es realizados, los árboles de decisión aprendidos en cada una de estas muestras, y sus predicciones combinados utilizando un voto mayoritario. Mientras que el Bagging ayuda a reducir el exceso de ajuste efectos, el hecho de que en lugar de un árbol se creen varios puede ser un impedimento a la interpretación de los clasificadores resultantes.
La idea de Bagging se lleva un paso más allá en la construcción de Random Forest. Las instancias no solo se vuelven a muestrear, sino que para cada prueba el conjunto de los atributos que se evalúan como posibles pruebas se eligen al azar. Estos dos Los efectos, además de utilizar los árboles sin podar, esencialmente aseguran que no se pueda entender nada más allá de las interacciones de atributos al interpretar el final.
Boosting, como lo encarna el conocido sistema AdaBoost. Por otro lado, induce de forma iterativa a los clasificadores débiles en los conjuntos de datos cuya distribución se modifica de acuerdo con el rendimiento de los clasificadores anteriores. Principalmente, esto toma la forma de remuestrear las instancias mal clasificadas, o volver a ponderarlas.
Si bien puede comprobarse que la técnica de refuerzo se aproxima a la subyacente real función de clasificación en un grado arbitrario, el conjunto resultante es de nuevo bastante difícil de interpretar, especialmente dados los cambios en la distribución subyacente que se efectúan durante el proceso de aprendizaje.
